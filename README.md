# ğŸ¯ Desafio de Prompt Engineering - Bug to User Story

Projeto de otimizaÃ§Ã£o de prompts para conversÃ£o automÃ¡tica de bug reports em User Stories completas e profissionais, atingindo mÃ©dia >= 0.9 em todas as mÃ©tricas de avaliaÃ§Ã£o.

## ğŸ† Status: âœ… APROVADO

**MÃ©dia geral:** **0.9764** (meta: >= 0.9000)

---

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa e valida um prompt otimizado que transforma bug reports tÃ©cnicos em User Stories Ã¡geis de alta qualidade, utilizando **6 tÃ©cnicas avanÃ§adas de Prompt Engineering**.

### CaracterÃ­sticas Principais

- âœ… **6 tÃ©cnicas avanÃ§adas** aplicadas e validadas
- âœ… **MÃ©dia 0.9764** em avaliaÃ§Ã£o de casos crÃ­ticos
- âœ… **Baixa variabilidade** (desvio padrÃ£o: 0.0054)
- âœ… **Alta consistÃªncia** entre execuÃ§Ãµes
- âœ… **Formato estruturado** com Markdown
- âœ… **CritÃ©rios testÃ¡veis** (Given-When-Then)

---

## ğŸ”¬ TÃ©cnicas de Prompt Engineering Aplicadas

1. **Role Prompting** - Persona de PM SÃªnior empÃ¡tico
2. **Emotional Priming** - AtivaÃ§Ã£o de empatia antes da escrita
3. **Chain of Thought** - 6 passos de raciocÃ­nio estruturado
4. **Rubric-based Prompting** - 5 dimensÃµes de avaliaÃ§Ã£o explÃ­citas
5. **Few-Shot Learning** - 7 exemplos (4 simples + 3 mÃ©dios)
6. **Negative Examples** - Contrastes entre boas e mÃ¡s prÃ¡ticas

Ver detalhes completos em [RESULTS.md](RESULTS.md).

---

## ğŸ“Š Resultados

### Scores Finais (v4.0)

| MÃ©trica | Score | Status |
|---------|-------|--------|
| F1-Score | **0.9805** | âœ… |
| Clarity | **0.9800** | âœ… |
| Precision | **0.9707** | âœ… |
| Helpfulness | **0.9753** | âœ… |
| Correctness | **0.9756** | âœ… |
| **MÃ©dia Geral** | **0.9764** | âœ… |

### EvoluÃ§Ã£o

- **v3.1 â†’ v4.0:** +12.5% na mÃ©dia geral
- **F1-Score:** +18.1% (maior melhoria)
- **Correctness:** +14.8%

Ver anÃ¡lise completa em [RESULTS.md](RESULTS.md).

---

## ğŸš€ Quick Start

### 1. InstalaÃ§Ã£o

```bash
# Criar ambiente
conda create -n prompt-opt python=3.12
conda activate prompt-opt

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. ConfiguraÃ§Ã£o

```bash
# Copiar template de configuraÃ§Ã£o
cp .env.example .env

# Editar com suas API keys
# - OPENAI_API_KEY
# - LANGSMITH_API_KEY
```

### 3. Executar AvaliaÃ§Ã£o

```bash
# AvaliaÃ§Ã£o rÃ¡pida (5 casos crÃ­ticos x 3 rodadas)
python evaluate_quick.py

# AvaliaÃ§Ã£o completa (10 casos)
python src/evaluate.py

# DiagnÃ³stico focado (comparaÃ§Ã£o linha por linha)
python diagnose_worst_cases.py
```

---

## Estrutura obrigatÃ³ria do projeto

FaÃ§a um fork do repositÃ³rio base: **[Clique aqui para o template](https://github.com/devfullcycle/mba-ia-pull-evaluation-prompt)**

```
desafio-prompt-engineer/
â”œâ”€â”€ .env.example              # Template das variÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ README.md                 # Sua documentaÃ§Ã£o do processo
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ bug_to_user_story_v1.yml       # Prompt inicial (apÃ³s pull)
â”‚   â””â”€â”€ bug_to_user_story_v2.yml # Seu prompt otimizado
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pull_prompts.py       # Pull do LangSmith
â”‚   â”œâ”€â”€ push_prompts.py       # Push ao LangSmith
â”‚   â”œâ”€â”€ evaluate.py           # AvaliaÃ§Ã£o automÃ¡tica
â”‚   â”œâ”€â”€ metrics.py            # 4 mÃ©tricas implementadas
â”‚   â”œâ”€â”€ dataset.py            # 15 exemplos de bugs
â”‚   â””â”€â”€ utils.py              # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_prompts.py       # Testes de validaÃ§Ã£o
â”‚
```

**O que vocÃª vai criar:**

- `prompts/bug_to_user_story_v2.yml` - Seu prompt otimizado
- `tests/test_prompts.py` - Seus testes de validaÃ§Ã£o
- `src/pull_prompt.py` Script de pull do repositÃ³rio da fullcycle
- `src/push_prompt.py` Script de push para o seu repositÃ³rio
- `README.md` - DocumentaÃ§Ã£o do seu processo de otimizaÃ§Ã£o

**O que jÃ¡ vem pronto:**

- Dataset com 15 bugs (5 simples, 7 mÃ©dios, 3 complexos)
- 4 mÃ©tricas especÃ­ficas para Bug to User Story
- Suporte multi-provider (OpenAI e Gemini)

## RepositÃ³rios Ãºteis

- [RepositÃ³rio boilerplate do desafio](https://github.com/devfullcycle/desafio-prompt-engineer/)
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

## VirtualEnv para Python

Crie e ative um ambiente virtual antes de instalar dependÃªncias:

```bash
python3 -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸ¯ Exemplo de Uso

### Input (Bug Report)

```
BotÃ£o de adicionar ao carrinho nÃ£o funciona no produto ID 1234.
```

### Output (User Story Gerada)

```markdown
Como um cliente navegando na loja, eu quero adicionar produtos ao meu carrinho 
de compras, para que eu possa continuar comprando e finalizar minha compra depois.

CritÃ©rios de AceitaÃ§Ã£o:
- Dado que estou visualizando um produto
- Quando clico no botÃ£o "Adicionar ao Carrinho"
- EntÃ£o o produto deve ser adicionado ao carrinho
- E devo ver uma confirmaÃ§Ã£o visual
- E o contador do carrinho deve ser atualizado
```

---

## ğŸ”§ Comandos Ãšteis

| AÃ§Ã£o | Comando |
|------|---------|
| Validar estrutura | `pytest tests/test_prompts.py -v` |
| Push para LangSmith | `python src/push_prompts.py` |
| AvaliaÃ§Ã£o completa | `python src/evaluate.py` |
| **AvaliaÃ§Ã£o rÃ¡pida** | `python evaluate_quick.py` âš¡ |
| **DiagnÃ³stico focado** | `python diagnose_worst_cases.py` ğŸ”¬ |
| Todos os testes | `pytest tests/ -v` |

---

## ğŸ“š DocumentaÃ§Ã£o

- **[RESULTS.md](RESULTS.md)** - Resultados completos, tÃ©cnicas e anÃ¡lise detalhada
- **[prompts/bug_to_user_story_v2.yml](prompts/bug_to_user_story_v2.yml)** - Prompt otimizado v4.0
- **[LangSmith Hub](https://smith.langchain.com/prompts/bug_to_user_story_v2/032a9885)** - Prompt pÃºblico

---

## ğŸ’¡ Highlights

### O que Torna Este Prompt Especial?

1. **Emotional Priming** - Ativa empatia antes da geraÃ§Ã£o
2. **Chain of Thought** - RaciocÃ­nio estruturado em 6 passos
3. **Rubric-based** - Modelo sabe exatamente como serÃ¡ avaliado
4. **Few-Shot Learning** - 7 exemplos cobrindo diferentes complexidades
5. **Negative Examples** - Mostra o que evitar (contrastes)
6. **Anti-AlucinaÃ§Ã£o** - 6 regras explÃ­citas para fidelidade aos dados

### MÃ©tricas Chave

- âœ… **98.05%** F1-Score (precisÃ£o + completude)
- âœ… **98.00%** Clarity (clareza perfeita em todas rodadas)
- âœ… **0.0054** desvio padrÃ£o (alta consistÃªncia)
- âœ… **100%** dos casos crÃ­ticos aprovados

---

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```bash
# OpenAI (obrigatÃ³rio)
OPENAI_API_KEY=sk-...

# LangSmith (obrigatÃ³rio)
LANGSMITH_API_KEY=lsv2_pt_...
LANGCHAIN_PROJECT=prompt-optimization-challenge-resolved

# ConfiguraÃ§Ã£o de LLM
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o
EVAL_MODEL=gpt-4o
```

### DependÃªncias Principais

- `langchain` >= 0.1.0
- `langsmith` >= 0.1.0
- `openai` >= 1.0.0
- `python-dotenv`
- `pyyaml`
- `pytest`

Ver versÃµes exatas em [requirements.txt](requirements.txt).

---

## ğŸ“ˆ Roadmap

### âœ… ConcluÃ­do

- [x] Implementar 6 tÃ©cnicas avanÃ§adas
- [x] Atingir mÃ©dia >= 0.9
- [x] Validar consistÃªncia (mÃºltiplas rodadas)
- [x] Documentar resultados completos
- [x] Criar scripts de diagnÃ³stico


